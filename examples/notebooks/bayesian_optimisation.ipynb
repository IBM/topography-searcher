{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf7b3df-f835-4cb8-941b-7410789832b9",
   "metadata": {},
   "source": [
    "# Topographical batch Bayesian optimisation\n",
    "\n",
    "In this notebook we show how to use TopSearch to generate the landscape of the acquisition functions generated in Bayesian optimisation. Bayesian optimisation is a framework to try and efficiently optimise functions that are expensive to evaluate. The process begins with an initial dataset to which we fit a surrogate model (Gaussian process) and construct an acquisition function that combines the mean and uncertainty of the Gaussian process to decide where is most useful to sample next in order to minimise the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb93c6c-39f3-4891-bc39-a33af629d671",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "First we import topsearch, which contains all the functionality we need for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5493a61-439c-4441-88c7-cce5d5a6ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import topsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44eef9e-45eb-4a02-b31b-b6c0be484d5d",
   "metadata": {},
   "source": [
    "## Initialise classes\n",
    "\n",
    "Landscape generation requires several classes to be instantiated. We list each of them in turn here, and describe their functionality.\n",
    "First, we generate the potential classes that calculates the function value and its derivatives when given a set of coordinates. In this case we need three different functions. One is the true function we are trying to optimise, the other is the log marginal likelihood surface for fitting the Gaussian Process. Finally we need an acquisition function that uses all the uncertainty and mean of the Gaussian process to specify where to sample next, we use the upper confidence bound in this example.\n",
    "The function we are aiming to optimise in this example is the two-dimensional Schwefel function within the standard range of -500 to 500 in all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e395c-1157-4556-a730-f365ecaa581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_bounds = [(-500.0, 500.0), (-500.0, 500.0)]\n",
    "hyperparameter_bounds = [(1e-2, 1.0), (1e-2, 1.0), (1e-6, 1e-1)]\n",
    "schwefel = topsearch.potential.Schwefel(ndim=2, bounds=function_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a15f5-2775-4013-b82d-141067f9dcb3",
   "metadata": {},
   "source": [
    "Next we initialise the log marginal likelihood surface that is explored during Gaussian process fitting. We provide it the data for fitting, which is the data. We specify the standard radial basis function kernel.  We normalise the training data to lie in the range (0,1). The ranges of allowed hyperparameters is specified for the lengthscales and the final element is the noise term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b2350-ef98-4914-bb89-1ee1c690dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lml = topsearch.potential.LogMarginalLikelihood(kernel_choice='RBF',\n",
    "                                                training=schwefel.training,\n",
    "                                                response=schwefel.response,\n",
    "                                                bounds=hyperparameter_bounds,\n",
    "                                                norm_training=True,\n",
    "                                                function_bounds=function_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105c92f-38b9-482a-b098-c6d6a4d5775a",
   "metadata": {},
   "source": [
    "Finally, we initialise the acquisition function, which uses the Gaussian process to estimate the utility in picking any given point as the next sample point when aiming to optimise the target function. We provide it the gaussian process, and the bounds of the target function for its sampling. We also provide it with the choice of acquisition function as many have been devised. Here, we use the upper confidence bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74c864-5c8a-4e19-b3a7-59dc7d1ee3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition = topsearch.potential.Acquisition(gaussian_process=lml,\n",
    "                                              bounds=function_bounds,\n",
    "                                              acquisition_choice='UCB',\n",
    "                                              zeta=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58b7952-87f3-4732-9ef3-6ed356021f86",
   "metadata": {},
   "source": [
    "Next we initialise the comparison class, which is responsible for determining the distance between points in feature space and determining if they are the same. We may generate repeated minima and transition states and the functions of this class will allow us to filter out repeats and only include unique stationary points. We are required to specify a distance_criterion and energy_criterion within which two minima and transition states are considered the same. The proportional_distance scales the distance_criterion to reflect the total range, in this case it is active resulting in the criterion taking any points within 5% of the feature space and within the energy_criterion as the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff976478-4838-46cb-9224-819070840de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer = topsearch.similarity.NonAtomicSimilarity(potential=acquisition,\n",
    "                                                    distance_criterion=0.05,\n",
    "                                                    energy_criterion=1e-2,\n",
    "                                                    proportional_distance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ba276-b4d9-4d0f-beff-0345a17b76af",
   "metadata": {},
   "source": [
    "We provide these two instances to the KineticTransitionNetwork class. This contains the networkx object in which all the minima and transition states are stored as a weighted graph. This class controls all access to the network and performs operations to extract information and analyse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94780e0f-48e3-4efa-a848-99e457d8f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ktn = topsearch.kinetic_transition_network.KineticTransitionNetwork(potential=acquisition,\n",
    "                                                                    similarity=comparer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655eaf61-4758-41c8-bcd1-4573bc63a6d3",
   "metadata": {},
   "source": [
    "Additionally, we need a local minimiser as all these methods rely on locating local minima. Our local minimiser class provides a wrapper to the scipy box-constrained LBFGS implementation that is adapted to the different optimisation tasks we perform. We need to provide it the function that it minimises, along with the maximum number of steps it can take, the LBFGS history and gradient at which it has successfully converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d327cd-45c4-455a-b613-aff57157db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimiser = topsearch.minimisation.LBFGS(potential=acquisition,\n",
    "                                         conv_crit=1e-6,\n",
    "                                         history_size=5,\n",
    "                                         n_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7730a-a1f6-4c21-a3c1-21e99b9424d7",
   "metadata": {},
   "source": [
    "Locating transition states requires a combination of single-ended and double-ended searches. Double-ended searches take two minima as input and attempt to find the lowest-valued path between them. Single-ended searches start from a single point and follow the eigenvector corresponding to the most negative eigenvalue until they converge to a transition state. These two searches are used in tandem, with an initial double-ended search, following by a single-ended search applied to each local maximum on the path. Here, we use the nudged elastic band as the double-ended search method. NudgedElasticBand contains the methods to produce an initial path and optimise it to minimise its overall value. We specify it using three parameters: force_constant - determines the tightness of the elastic band, this is updated within the computation, image density - this determines how many points the path is composed of (per unit distance), more points means better path (usually) as higher computational cost, max images - allows us to put a cut on the maximum number of images to limit the computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cbed3-c07a-4d6d-b7ad-dfb0a4e1be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nudged_elastic_band = \\\n",
    "    topsearch.double_ended_search.NudgedElasticBand(potential=acquisition,\n",
    "                                                    minimiser=minimiser,\n",
    "                                                    force_constant=50.0,\n",
    "                                                    image_density=5.0,\n",
    "                                                    max_images=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ed7cd-2986-41cc-8293-f48f60f35c0d",
   "metadata": {},
   "source": [
    "The single-ended search method is hybrid eigenvector-following. This class provides methods to take in a single point and provide the methods to converge it to the nearest transition state. We provide it with a convergence criterion, the gradient must be under this value to be considered converged to a stationary point. We also provide the allowed number of mode-following steps before it is considered a failed search for a transition state. Each transition state is connected to two minima by following the steepest-descent paths along the unique downhill direction and we specify the distance we move in the downhill direction before beginning a local minimisation using pushoff. Finally, we have some step sizes used in the mode-following to prevent the steps being too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9869fb6a-28f2-4324-8b9b-b1c017f3f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_eigenvector_following = \\\n",
    "    topsearch.single_ended_search.HybridEigenvectorFollowing(potential=acquisition,\n",
    "                                                             minimiser=minimiser,\n",
    "                                                             conv_crit=1e-4,\n",
    "                                                             ts_steps=100,\n",
    "                                                             pushoff=1e-1,\n",
    "                                                             max_uphill_step_size=0.5,\n",
    "                                                             positive_eigenvalue_step=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c0969-2938-4ce6-8b87-b9c47a5e1f40",
   "metadata": {},
   "source": [
    "For global optimisation algorithms we need to be able to propose new structures from the existing ones. The efficiency of global optimisation relies on the proposal of good candidate positions. For molecular systems this is an involved problem with a lot of research invested into it. For the non-atomic system here the step-taking is much simpler, it can just be random perturbations. This class manages the perturbations to propose new states, it is given the maximum step size, max_displacement, and we specify that this distance should be measured as a proportion of the bounds range with proportional distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5e17a-f822-4a33-b442-d8e67389130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_taking = topsearch.perturbations.NonAtomicPerturbation(potential=acquisition,\n",
    "                                                            max_displacement=1.0,\n",
    "                                                            proportional_distance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ba991-65ed-4b28-b4c8-b75770f6ec71",
   "metadata": {},
   "source": [
    "Initialise the global optimisation class. The global optimisation algorithm is basin-hopping, which is provided with the step-taking class previously created. Basin-hopping steps around the surface performing local minimisations and subsequently accepting or rejecting the new local minima based on a Metropolis-like criterion. The BasinHopping class performs basin-hopping runs consisting of n_steps random perturbations and local minimisations, with a temperature specified to control the acceptance of new minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761bbc0-adcf-414a-b931-ccc02469730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = topsearch.global_optimisation.BasinHopping(ktn=ktn,\n",
    "                                                       minimiser=minimiser,\n",
    "                                                       n_steps=750,\n",
    "                                                       temperature=1.0,\n",
    "                                                       step_taking=step_taking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d3b42-d929-492d-b6d8-56faf7ba605b",
   "metadata": {},
   "source": [
    "Finally, we feed many of these objects into a NetworkSampling object that controls all the landscape generation. This object allows for simple calls to be made that perform the combination of algorithms for landscape generation. We pass it the global_optimiser and the transition state location methods. Transition state location simply requires two minima, and therefore, the sampling of the landscape is embarrassingly parallel. We have the option to use multiple processes to accelerate the landscape calculation. Here we decide to run on one CPU for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae62e04-d97d-43e2-93b6-e6f51d9f0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer = topsearch.exploration.NetworkSampling(ktn=ktn,\n",
    "                                                 minimiser=minimiser,\n",
    "                                                 global_optimiser=optimiser,\n",
    "                                                 single_ended_search=hybrid_eigenvector_following,\n",
    "                                                 double_ended_search=nudged_elastic_band,\n",
    "                                                 multiprocessing=False,\n",
    "                                                 n_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d28603-fdf1-4aee-a369-fa0e3e8be398",
   "metadata": {},
   "source": [
    "## 1. Construct an acquisition function\n",
    "\n",
    "First we will read in the dataset provided, fit a Gaussian process and construct the acquisition function surface used in Bayesian optimisation. The chosen acquisition function is the upper confidence bound. We first read in the initial datasets and fit a Gaussian process regression model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0344817e-5423-4a64-a610-f680acba3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lml.read_data('training.txt','response.txt')\n",
    "lml.get_new_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038bc72b-a679-4bbc-bcca-e838dc842e23",
   "metadata": {},
   "source": [
    "With the Gaussian process fit it is possible to read in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf93b4-5e5a-4554-bc14-14077855f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition.generate_network('ClosestEnumeration', 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
